#!/usr/bin/env python3
"""
å®éªŒç»“æœå¯è§†åŒ–å·¥å…·
ä»TensorBoardæ—¥å¿—å’Œç»“æœæ–‡ä»¶ä¸­ç”ŸæˆæŠ¥å‘Šå›¾è¡¨
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
from config import Config

class ExperimentVisualizer:
    """å®éªŒç»“æœå¯è§†åŒ–å™¨"""
    
    def __init__(self, results_dir='results', tensorboard_dir='tensorboard_logs'):
        self.results_dir = results_dir
        self.tensorboard_dir = tensorboard_dir
        self.output_dir = 'visualization_outputs'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œæ ·å¼
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['figure.figsize'] = (12, 8)
        plt.rcParams['font.size'] = 12
        
    def extract_tensorboard_data(self, experiment_name):
        """ä»TensorBoardæ—¥å¿—ä¸­æå–æ•°æ®"""
        log_dir = os.path.join(self.tensorboard_dir, experiment_name)
        
        if not os.path.exists(log_dir):
            print(f"Warning: TensorBoard log directory not found: {log_dir}")
            return None
            
        try:
            ea = EventAccumulator(log_dir)
            ea.Reload()
            
            # æå–æ ‡é‡æ•°æ®
            scalar_tags = ea.Tags()['scalars']
            
            data = {}
            for tag in scalar_tags:
                scalar_events = ea.Scalars(tag)
                data[tag] = {
                    'steps': [event.step for event in scalar_events],
                    'values': [event.value for event in scalar_events]
                }
            
            return data
            
        except Exception as e:
            print(f"Error extracting TensorBoard data for {experiment_name}: {e}")
            return None
    
    def plot_training_curves(self, experiment_names=None):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if experiment_names is None:
            # ä»resultsç›®å½•è‡ªåŠ¨è·å–å®éªŒåç§°
            result_files = [f for f in os.listdir(self.results_dir) 
                           if f.endswith('_results.json')]
            experiment_names = [f.replace('_results.json', '') for f in result_files]
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Training Progress Comparison', fontsize=16, fontweight='bold')
        
        colors = plt.cm.Set1(np.linspace(0, 1, len(experiment_names)))
        
        for idx, exp_name in enumerate(experiment_names):
            color = colors[idx]
            
            # ä»JSONç»“æœæ–‡ä»¶è¯»å–æ•°æ®
            result_file = os.path.join(self.results_dir, f"{exp_name}_results.json")
            
            if os.path.exists(result_file):
                with open(result_file, 'r') as f:
                    data = json.load(f)
                
                history = data.get('train_history', {})
                
                epochs = list(range(1, len(history.get('train_losses', [])) + 1))
                
                # è®­ç»ƒæŸå¤±
                if 'train_losses' in history:
                    axes[0, 0].plot(epochs, history['train_losses'], 
                                   color=color, linestyle='-', linewidth=2,
                                   label=f"{exp_name} (Train)", alpha=0.8)
                
                # éªŒè¯æŸå¤±
                if 'val_losses' in history:
                    axes[0, 0].plot(epochs, history['val_losses'], 
                                   color=color, linestyle='--', linewidth=2,
                                   label=f"{exp_name} (Val)", alpha=0.8)
                
                # è®­ç»ƒå‡†ç¡®ç‡
                if 'train_accuracies' in history:
                    axes[0, 1].plot(epochs, history['train_accuracies'], 
                                   color=color, linestyle='-', linewidth=2,
                                   label=f"{exp_name} (Train)", alpha=0.8)
                
                # éªŒè¯å‡†ç¡®ç‡
                if 'val_accuracies' in history:
                    axes[0, 1].plot(epochs, history['val_accuracies'], 
                                   color=color, linestyle='--', linewidth=2,
                                   label=f"{exp_name} (Val)", alpha=0.8)
        
        # è®¾ç½®å­å›¾æ ‡é¢˜å’Œæ ‡ç­¾
        axes[0, 0].set_title('Loss Curves', fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        axes[0, 0].grid(True, alpha=0.3)
        
        axes[0, 1].set_title('Accuracy Curves', fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy (%)')
        axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        axes[0, 1].grid(True, alpha=0.3)
        
        # æœ€ç»ˆæ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
        self._plot_final_performance_bars(axes[1, 0], axes[1, 1], experiment_names)
        
        plt.tight_layout()
        output_file = os.path.join(self.output_dir, 'training_curves_comparison.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Training curves saved to: {output_file}")
        return output_file
    
    def _plot_final_performance_bars(self, ax1, ax2, experiment_names):
        """ç»˜åˆ¶æœ€ç»ˆæ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾"""
        val_accs = []
        test_accs = []
        exp_labels = []
        
        for exp_name in experiment_names:
            result_file = os.path.join(self.results_dir, f"{exp_name}_results.json")
            
            if os.path.exists(result_file):
                with open(result_file, 'r') as f:
                    data = json.load(f)
                
                val_accs.append(data.get('best_val_acc', 0))
                test_accs.append(data.get('test_acc', 0))
                exp_labels.append(exp_name.replace('_', '\n'))
        
        if val_accs and test_accs:
            x = np.arange(len(exp_labels))
            width = 0.35
            
            bars1 = ax1.bar(x - width/2, val_accs, width, label='Validation Acc', 
                           color='skyblue', alpha=0.8)
            bars2 = ax1.bar(x + width/2, test_accs, width, label='Test Acc', 
                           color='lightcoral', alpha=0.8)
            
            ax1.set_title('Final Accuracy Comparison', fontweight='bold')
            ax1.set_ylabel('Accuracy (%)')
            ax1.set_xticks(x)
            ax1.set_xticklabels(exp_labels, rotation=45, ha='right')
            ax1.legend()
            ax1.grid(True, alpha=0.3, axis='y')
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar in bars1:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{height:.1f}%', ha='center', va='bottom', fontsize=10)
            
            for bar in bars2:
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{height:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # é¢„è®­ç»ƒå½±å“åˆ†æ
        self._plot_pretraining_impact(ax2, experiment_names)
    
    def _plot_pretraining_impact(self, ax, experiment_names):
        """ç»˜åˆ¶é¢„è®­ç»ƒå½±å“åˆ†æ"""
        pretrained_accs = []
        scratch_accs = []
        
        for exp_name in experiment_names:
            result_file = os.path.join(self.results_dir, f"{exp_name}_results.json")
            
            if os.path.exists(result_file):
                with open(result_file, 'r') as f:
                    data = json.load(f)
                
                config = data.get('experiment_config', {})
                test_acc = data.get('test_acc', 0)
                
                if config.get('pretrained', False):
                    pretrained_accs.append(test_acc)
                else:
                    scratch_accs.append(test_acc)
        
        if pretrained_accs and scratch_accs:
            categories = ['Pretrained', 'From Scratch']
            means = [np.mean(pretrained_accs), np.mean(scratch_accs)]
            stds = [np.std(pretrained_accs), np.std(scratch_accs)]
            
            bars = ax.bar(categories, means, yerr=stds, capsize=5, 
                         color=['green', 'orange'], alpha=0.7)
            
            ax.set_title('Pretrained vs From Scratch', fontweight='bold')
            ax.set_ylabel('Test Accuracy (%)')
            ax.grid(True, alpha=0.3, axis='y')
            
            # æ˜¾ç¤ºæ•°å€¼
            for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):
                ax.text(bar.get_x() + bar.get_width()/2., mean + std + 1,
                       f'{mean:.1f}Â±{std:.1f}%', ha='center', va='bottom', 
                       fontweight='bold')
            
            # æ˜¾ç¤ºæ”¹è¿›å¹…åº¦
            if len(means) == 2:
                improvement = means[0] - means[1]
                ax.text(0.5, max(means) + max(stds) + 5,
                       f'Improvement: +{improvement:.1f}%', 
                       ha='center', va='bottom', fontweight='bold',
                       bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
        else:
            ax.text(0.5, 0.5, 'Insufficient data for\npretraining comparison', 
                   ha='center', va='center', transform=ax.transAxes,
                   fontsize=12, style='italic')
            ax.set_title('Pretrained vs From Scratch', fontweight='bold')
    
    def plot_confusion_matrices(self):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        result_files = [f for f in os.listdir(self.results_dir) 
                       if f.endswith('_results.json')]
        
        for result_file in result_files:
            exp_name = result_file.replace('_results.json', '')
            
            with open(os.path.join(self.results_dir, result_file), 'r') as f:
                data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ··æ·†çŸ©é˜µæ•°æ®
            if 'confusion_matrix' in data:
                cm = np.array(data['confusion_matrix'])
                
                plt.figure(figsize=(10, 8))
                sns.heatmap(cm, annot=False, cmap='Blues', fmt='d')
                plt.title(f'Confusion Matrix - {exp_name}', fontweight='bold')
                plt.xlabel('Predicted Label')
                plt.ylabel('True Label')
                
                output_file = os.path.join(self.output_dir, f'{exp_name}_confusion_matrix.png')
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                plt.close()
                
                print(f"Confusion matrix saved: {output_file}")
    
    def generate_performance_summary_plot(self):
        """ç”Ÿæˆæ€§èƒ½æ€»ç»“å›¾"""
        result_files = [f for f in os.listdir(self.results_dir) 
                       if f.endswith('_results.json')]
        
        if not result_files:
            print("No result files found!")
            return
        
        data_rows = []
        
        for result_file in result_files:
            with open(os.path.join(self.results_dir, result_file), 'r') as f:
                data = json.load(f)
            
            exp_name = data['experiment_name']
            config = data.get('experiment_config', {})
            
            # è§£æå®éªŒä¿¡æ¯
            parts = exp_name.split('_')
            model_name = parts[0]
            exp_type = '_'.join(parts[1:])
            
            data_rows.append({
                'Experiment': exp_name,
                'Model': model_name,
                'Type': exp_type,
                'Pretrained': config.get('pretrained', False),
                'Val Acc': data.get('best_val_acc', 0),
                'Test Acc': data.get('test_acc', 0),
                'Epochs': len(data.get('train_history', {}).get('train_losses', [])),
                'LR': config.get('learning_rate', 0)
            })
        
        df = pd.DataFrame(data_rows)
        
        # åˆ›å»ºå¤šå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Comprehensive Performance Analysis', fontsize=16, fontweight='bold')
        
        # 1. æ¨¡å‹å¯¹æ¯”
        model_performance = df.groupby('Model')['Test Acc'].agg(['mean', 'std']).reset_index()
        axes[0, 0].bar(model_performance['Model'], model_performance['mean'], 
                      yerr=model_performance['std'], capsize=5, alpha=0.7)
        axes[0, 0].set_title('Model Comparison (Test Accuracy)', fontweight='bold')
        axes[0, 0].set_ylabel('Test Accuracy (%)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3, axis='y')
        
        # 2. å­¦ä¹ ç‡å½±å“
        lr_performance = df.groupby('LR')['Test Acc'].mean().reset_index()
        axes[0, 1].plot(lr_performance['LR'], lr_performance['Test Acc'], 'o-', linewidth=2, markersize=8)
        axes[0, 1].set_title('Learning Rate Impact', fontweight='bold')
        axes[0, 1].set_xlabel('Learning Rate')
        axes[0, 1].set_ylabel('Test Accuracy (%)')
        axes[0, 1].set_xscale('log')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. è®­ç»ƒè½®æ•°ä¸æ€§èƒ½
        axes[1, 0].scatter(df['Epochs'], df['Test Acc'], c=df['Pretrained'].map({True: 'red', False: 'blue'}), 
                          alpha=0.7, s=100)
        axes[1, 0].set_title('Epochs vs Performance', fontweight='bold')
        axes[1, 0].set_xlabel('Training Epochs')
        axes[1, 0].set_ylabel('Test Accuracy (%)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ·»åŠ å›¾ä¾‹
        red_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Pretrained')
        blue_patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='From Scratch')
        axes[1, 0].legend(handles=[red_patch, blue_patch])
        
        # 4. éªŒè¯vsæµ‹è¯•å‡†ç¡®ç‡
        axes[1, 1].scatter(df['Val Acc'], df['Test Acc'], alpha=0.7, s=100)
        axes[1, 1].plot([df['Val Acc'].min(), df['Val Acc'].max()], 
                       [df['Val Acc'].min(), df['Val Acc'].max()], 'r--', alpha=0.5)
        axes[1, 1].set_title('Validation vs Test Accuracy', fontweight='bold')
        axes[1, 1].set_xlabel('Validation Accuracy (%)')
        axes[1, 1].set_ylabel('Test Accuracy (%)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        output_file = os.path.join(self.output_dir, 'performance_analysis.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Performance analysis saved: {output_file}")
        
        # ä¿å­˜æ•°æ®è¡¨
        df.to_csv(os.path.join(self.output_dir, 'performance_summary.csv'), index=False)
        print(f"Performance data saved: {os.path.join(self.output_dir, 'performance_summary.csv')}")
        
        return output_file
    
    def generate_all_visualizations(self):
        """ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        print("Generating comprehensive visualizations...")
        
        outputs = []
        
        # 1. è®­ç»ƒæ›²çº¿
        print("\n1. Generating training curves...")
        try:
            output = self.plot_training_curves()
            outputs.append(output)
        except Exception as e:
            print(f"Error generating training curves: {e}")
        
        # 2. æ€§èƒ½åˆ†æ
        print("\n2. Generating performance analysis...")
        try:
            output = self.generate_performance_summary_plot()
            outputs.append(output)
        except Exception as e:
            print(f"Error generating performance analysis: {e}")
        
        # 3. æ··æ·†çŸ©é˜µ
        print("\n3. Generating confusion matrices...")
        try:
            self.plot_confusion_matrices()
        except Exception as e:
            print(f"Error generating confusion matrices: {e}")
        
        print(f"\nâœ… All visualizations saved to: {self.output_dir}/")
        return outputs

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ Starting experiment visualization...")
    
    visualizer = ExperimentVisualizer()
    outputs = visualizer.generate_all_visualizations()
    
    print(f"\nğŸ‰ Visualization complete!")
    print(f"ğŸ“ Output directory: {visualizer.output_dir}/")
    print(f"ğŸ“Š Generated files:")
    
    for output_file in os.listdir(visualizer.output_dir):
        print(f"   - {output_file}")
    
    print(f"\nğŸ“‹ Next steps:")
    print(f"1. Review generated charts")
    print(f"2. Include in your report")
    print(f"3. Start TensorBoard for interactive analysis:")
    print(f"   tensorboard --logdir {Config.TENSORBOARD_LOG_DIR}")

if __name__ == '__main__':
    main() 